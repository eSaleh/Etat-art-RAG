{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10560148,"sourceType":"datasetVersion","datasetId":6533526},{"sourceId":165426226,"sourceType":"kernelVersion"},{"sourceId":218995722,"sourceType":"kernelVersion"},{"sourceId":11372,"sourceType":"modelInstanceVersion","modelInstanceId":5388,"modelId":3533}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U accelerate bitsandbytes langchain langchain-community sentence-transformers ragatouille faiss-gpu rank_bm25\n# ! pip install -q -U beautifulsoup4 # Install beautifulsoup4 if you are running the notebook not in Kaggle\n!pip install -q -U keras-nlp\n!pip install -q -U keras>3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:43:19.641409Z","iopub.execute_input":"2025-01-24T13:43:19.641773Z","iopub.status.idle":"2025-01-24T13:43:33.267879Z","shell.execute_reply.started":"2025-01-24T13:43:19.641742Z","shell.execute_reply":"2025-01-24T13:43:33.266377Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import os\nimport keras\nimport keras_nlp\nimport pandas as pd\n\nfrom bs4 import BeautifulSoup\nfrom typing import Optional, List, Tuple\nfrom IPython.display import display, Markdown\n\nfrom transformers import AutoTokenizer\nfrom ragatouille import RAGPretrainedModel\nfrom langchain.docstore.document import Document\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain_core.runnables import ConfigurableField\nfrom langchain_community.vectorstores import FAISS, Chroma\nfrom langchain.retrievers import BM25Retriever, EnsembleRetriever\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import DataFrameLoader\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores.utils import DistanceStrategy\n\nos.environ[\"KERAS_BACKEND\"] = \"torch\"  # Or \"torch\" or \"tensorflow\".\n#os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\" # Avoid memory fragmentation on JAX backend.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:43:33.269963Z","iopub.execute_input":"2025-01-24T13:43:33.270264Z","iopub.status.idle":"2025-01-24T13:43:33.277357Z","shell.execute_reply.started":"2025-01-24T13:43:33.270238Z","shell.execute_reply":"2025-01-24T13:43:33.276389Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"\n\ndata = pd.read_csv('/kaggle/input/kaggle-solutions-methods/kaggle_winning_solutions_methods.csv')\ndata.head()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:43:33.278994Z","iopub.execute_input":"2025-01-24T13:43:33.279311Z","iopub.status.idle":"2025-01-24T13:43:34.054449Z","shell.execute_reply.started":"2025-01-24T13:43:33.279273Z","shell.execute_reply":"2025-01-24T13:43:34.053284Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"                                                link  place  \\\n0  https://www.kaggle.com/c/asl-signs/discussion/...      2   \n1  https://www.kaggle.com/c/asl-signs/discussion/...      2   \n2  https://www.kaggle.com/c/asl-signs/discussion/...      2   \n3  https://www.kaggle.com/c/asl-signs/discussion/...      2   \n4  https://www.kaggle.com/c/asl-signs/discussion/...      2   \n\n                              competition_name     prize   team      kind  \\\n0  Google - Isolated Sign Language Recognition  $100,000  1,165  Research   \n1  Google - Isolated Sign Language Recognition  $100,000  1,165  Research   \n2  Google - Isolated Sign Language Recognition  $100,000  1,165  Research   \n3  Google - Isolated Sign Language Recognition  $100,000  1,165  Research   \n4  Google - Isolated Sign Language Recognition  $100,000  1,165  Research   \n\n                    metric  year      nm  \\\n0  PostProcessorKernelDesc  2023  406306   \n1  PostProcessorKernelDesc  2023  406306   \n2  PostProcessorKernelDesc  2023  406306   \n3  PostProcessorKernelDesc  2023  406306   \n4  PostProcessorKernelDesc  2023  406306   \n\n                                             writeup  num_tokens  \\\n0  <h2>TLDR</h2>\\n<p>We used an approach similar ...        2914   \n1  <h2>TLDR</h2>\\n<p>We used an approach similar ...        2914   \n2  <h2>TLDR</h2>\\n<p>We used an approach similar ...        2914   \n3  <h2>TLDR</h2>\\n<p>We used an approach similar ...        2914   \n4  <h2>TLDR</h2>\\n<p>We used an approach similar ...        2914   \n\n                                             methods       cleaned_methods  \n0  ['EfficientNet-B0', 'Data Augmentation', 'Norm...  Replace augmentation  \n1  ['EfficientNet-B0', 'Data Augmentation', 'Norm...    Finger tree rotate  \n2  ['EfficientNet-B0', 'Data Augmentation', 'Norm...     Data Augmentation  \n3  ['EfficientNet-B0', 'Data Augmentation', 'Norm...    Onecycle scheduler  \n4  ['EfficientNet-B0', 'Data Augmentation', 'Norm...             Flip pose  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>link</th>\n      <th>place</th>\n      <th>competition_name</th>\n      <th>prize</th>\n      <th>team</th>\n      <th>kind</th>\n      <th>metric</th>\n      <th>year</th>\n      <th>nm</th>\n      <th>writeup</th>\n      <th>num_tokens</th>\n      <th>methods</th>\n      <th>cleaned_methods</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://www.kaggle.com/c/asl-signs/discussion/...</td>\n      <td>2</td>\n      <td>Google - Isolated Sign Language Recognition</td>\n      <td>$100,000</td>\n      <td>1,165</td>\n      <td>Research</td>\n      <td>PostProcessorKernelDesc</td>\n      <td>2023</td>\n      <td>406306</td>\n      <td>&lt;h2&gt;TLDR&lt;/h2&gt;\\n&lt;p&gt;We used an approach similar ...</td>\n      <td>2914</td>\n      <td>['EfficientNet-B0', 'Data Augmentation', 'Norm...</td>\n      <td>Replace augmentation</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://www.kaggle.com/c/asl-signs/discussion/...</td>\n      <td>2</td>\n      <td>Google - Isolated Sign Language Recognition</td>\n      <td>$100,000</td>\n      <td>1,165</td>\n      <td>Research</td>\n      <td>PostProcessorKernelDesc</td>\n      <td>2023</td>\n      <td>406306</td>\n      <td>&lt;h2&gt;TLDR&lt;/h2&gt;\\n&lt;p&gt;We used an approach similar ...</td>\n      <td>2914</td>\n      <td>['EfficientNet-B0', 'Data Augmentation', 'Norm...</td>\n      <td>Finger tree rotate</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://www.kaggle.com/c/asl-signs/discussion/...</td>\n      <td>2</td>\n      <td>Google - Isolated Sign Language Recognition</td>\n      <td>$100,000</td>\n      <td>1,165</td>\n      <td>Research</td>\n      <td>PostProcessorKernelDesc</td>\n      <td>2023</td>\n      <td>406306</td>\n      <td>&lt;h2&gt;TLDR&lt;/h2&gt;\\n&lt;p&gt;We used an approach similar ...</td>\n      <td>2914</td>\n      <td>['EfficientNet-B0', 'Data Augmentation', 'Norm...</td>\n      <td>Data Augmentation</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://www.kaggle.com/c/asl-signs/discussion/...</td>\n      <td>2</td>\n      <td>Google - Isolated Sign Language Recognition</td>\n      <td>$100,000</td>\n      <td>1,165</td>\n      <td>Research</td>\n      <td>PostProcessorKernelDesc</td>\n      <td>2023</td>\n      <td>406306</td>\n      <td>&lt;h2&gt;TLDR&lt;/h2&gt;\\n&lt;p&gt;We used an approach similar ...</td>\n      <td>2914</td>\n      <td>['EfficientNet-B0', 'Data Augmentation', 'Norm...</td>\n      <td>Onecycle scheduler</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://www.kaggle.com/c/asl-signs/discussion/...</td>\n      <td>2</td>\n      <td>Google - Isolated Sign Language Recognition</td>\n      <td>$100,000</td>\n      <td>1,165</td>\n      <td>Research</td>\n      <td>PostProcessorKernelDesc</td>\n      <td>2023</td>\n      <td>406306</td>\n      <td>&lt;h2&gt;TLDR&lt;/h2&gt;\\n&lt;p&gt;We used an approach similar ...</td>\n      <td>2914</td>\n      <td>['EfficientNet-B0', 'Data Augmentation', 'Norm...</td>\n      <td>Flip pose</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"\n\ndata['writeup'][42]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:43:34.056424Z","iopub.execute_input":"2025-01-24T13:43:34.056889Z","iopub.status.idle":"2025-01-24T13:43:34.063178Z","shell.execute_reply.started":"2025-01-24T13:43:34.056844Z","shell.execute_reply":"2025-01-24T13:43:34.062216Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'<p>Here is a quick overview of the 5th-place solution.</p>\\n<ol>\\n<li><p><strong>we applied various augmentations like flip, concatenation, etc</strong><br>\\n1.1. By applying different augmentations, we can increase the cv by ~ 0.02 (0.76 -&gt; 0.78)</p></li>\\n<li><p><strong>the model is only a transformer model based on the public kernels</strong><br>\\n2.1. By increasing the number of parameters, the performance of a single model can be increased to around 0.8 (0.78-&gt;0.8) in public LB.<br>\\n2.1.1. 3 layers of transformer with the embedding size 480.</p></li>\\n<li><p><strong>Preprocessing by mean and std of single sign sequence</strong><br>\\n3.1. the preprocessing does affect the final performance. <br>\\n3.1.1. we tried different ways of calculating the mean and std and found out that using the mean and std of the single sign sequence results in better cv.</p></li>\\n<li><p><strong>Feature engineering like distances between points</strong><br>\\n4.1. we selected and used around 106 points (as the public notebook by Heck).<br>\\n4.2. distances withinpoints of hands/nose/eyes/… are calculated.</p></li>\\n<li><p><strong>some methods to prevent overfitting like awp, random mask of frames, ema, etc …</strong></p></li>\\n</ol>\\n<p>many thanks to my teammates  <a href=\"https://www.kaggle.com/qiaoshiji\" target=\"_blank\">@qiaoshiji</a> <a href=\"https://www.kaggle.com/zengzhaoyang\" target=\"_blank\">@zengzhaoyang</a></p>\\n<p>The source code for training models can be found here : <a href=\"https://github.com/zhouyuanzhe/kaggleasl5thplacesolution\" target=\"_blank\">https://github.com/zhouyuanzhe/kaggleasl5thplacesolution</a></p>'"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"\n\n%%time\n\ndef clean_html(html_content):\n    \"\"\"Function to clean up HTML tags in each writeup\"\"\"\n    soup = BeautifulSoup(html_content, 'html.parser')\n    # Use '\\n' as a separator to preserve the structure of the various parts\n    text = soup.get_text(separator='\\n', strip=True)\n    return text\n\ndata['writeup'] = data['writeup'].apply(clean_html) # This might take a while\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:43:34.064115Z","iopub.execute_input":"2025-01-24T13:43:34.064369Z","iopub.status.idle":"2025-01-24T13:44:04.171837Z","shell.execute_reply.started":"2025-01-24T13:43:34.064348Z","shell.execute_reply":"2025-01-24T13:44:04.170834Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 30 s, sys: 109 ms, total: 30.1 s\nWall time: 30.1 s\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(data['writeup'][42])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:44:04.172677Z","iopub.execute_input":"2025-01-24T13:44:04.172911Z","iopub.status.idle":"2025-01-24T13:44:04.178031Z","shell.execute_reply.started":"2025-01-24T13:44:04.172891Z","shell.execute_reply":"2025-01-24T13:44:04.177020Z"}},"outputs":[{"name":"stdout","text":"Here is a quick overview of the 5th-place solution.\nwe applied various augmentations like flip, concatenation, etc\n1.1. By applying different augmentations, we can increase the cv by ~ 0.02 (0.76 -> 0.78)\nthe model is only a transformer model based on the public kernels\n2.1. By increasing the number of parameters, the performance of a single model can be increased to around 0.8 (0.78->0.8) in public LB.\n2.1.1. 3 layers of transformer with the embedding size 480.\nPreprocessing by mean and std of single sign sequence\n3.1. the preprocessing does affect the final performance.\n3.1.1. we tried different ways of calculating the mean and std and found out that using the mean and std of the single sign sequence results in better cv.\nFeature engineering like distances between points\n4.1. we selected and used around 106 points (as the public notebook by Heck).\n4.2. distances withinpoints of hands/nose/eyes/… are calculated.\nsome methods to prevent overfitting like awp, random mask of frames, ema, etc …\nmany thanks to my teammates\n@qiaoshiji\n@zengzhaoyang\nThe source code for training models can be found here :\nhttps://github.com/zhouyuanzhe/kaggleasl5thplacesolution\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"\n\ndata['LLM_context'] = (\n    \"Competition Name: \" + data['competition_name'] +\n    \",\\nPlace: \" + data['place'].astype(str) +\n    \",\\nMethods Used: \" + data['methods'] +\n    \",\\nSolution: \" + data['writeup']\n)\n\nprint(data['LLM_context'][42])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:44:04.178888Z","iopub.execute_input":"2025-01-24T13:44:04.179150Z","iopub.status.idle":"2025-01-24T13:44:04.294555Z","shell.execute_reply.started":"2025-01-24T13:44:04.179114Z","shell.execute_reply":"2025-01-24T13:44:04.293448Z"}},"outputs":[{"name":"stdout","text":"Competition Name: Google - Isolated Sign Language Recognition,\nPlace: 5,\nMethods Used: ['Augmentation', 'Transformer model', 'Preprocessing', 'Feature engineering', 'Overfitting prevention'],\nSolution: Here is a quick overview of the 5th-place solution.\nwe applied various augmentations like flip, concatenation, etc\n1.1. By applying different augmentations, we can increase the cv by ~ 0.02 (0.76 -> 0.78)\nthe model is only a transformer model based on the public kernels\n2.1. By increasing the number of parameters, the performance of a single model can be increased to around 0.8 (0.78->0.8) in public LB.\n2.1.1. 3 layers of transformer with the embedding size 480.\nPreprocessing by mean and std of single sign sequence\n3.1. the preprocessing does affect the final performance.\n3.1.1. we tried different ways of calculating the mean and std and found out that using the mean and std of the single sign sequence results in better cv.\nFeature engineering like distances between points\n4.1. we selected and used around 106 points (as the public notebook by Heck).\n4.2. distances withinpoints of hands/nose/eyes/… are calculated.\nsome methods to prevent overfitting like awp, random mask of frames, ema, etc …\nmany thanks to my teammates\n@qiaoshiji\n@zengzhaoyang\nThe source code for training models can be found here :\nhttps://github.com/zhouyuanzhe/kaggleasl5thplacesolution\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"data = data.drop(\"writeup\", axis=1) # We remove 'writeup' column as it is already in LLM_context","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:44:04.297685Z","iopub.execute_input":"2025-01-24T13:44:04.297920Z","iopub.status.idle":"2025-01-24T13:44:04.312861Z","shell.execute_reply.started":"2025-01-24T13:44:04.297900Z","shell.execute_reply":"2025-01-24T13:44:04.311890Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"loader = DataFrameLoader(data, page_content_column=\"LLM_context\")\ndocs = loader.load()\ndocs_subset = docs[:1500] # Part of the data is used to reduce execution time.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:44:04.314410Z","iopub.execute_input":"2025-01-24T13:44:04.314735Z","iopub.status.idle":"2025-01-24T13:44:06.000950Z","shell.execute_reply.started":"2025-01-24T13:44:04.314701Z","shell.execute_reply":"2025-01-24T13:44:06.000126Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"print(\"-----------PAGE CONTENT-----------\")\nprint(docs_subset[42].page_content)\nprint(\"\\n\\n-----------METADATA-----------\\n\")\nprint(docs_subset[42].metadata)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:44:06.001882Z","iopub.execute_input":"2025-01-24T13:44:06.002216Z","iopub.status.idle":"2025-01-24T13:44:06.009461Z","shell.execute_reply.started":"2025-01-24T13:44:06.002182Z","shell.execute_reply":"2025-01-24T13:44:06.008278Z"}},"outputs":[{"name":"stdout","text":"-----------PAGE CONTENT-----------\nCompetition Name: Google - Isolated Sign Language Recognition,\nPlace: 5,\nMethods Used: ['Augmentation', 'Transformer model', 'Preprocessing', 'Feature engineering', 'Overfitting prevention'],\nSolution: Here is a quick overview of the 5th-place solution.\nwe applied various augmentations like flip, concatenation, etc\n1.1. By applying different augmentations, we can increase the cv by ~ 0.02 (0.76 -> 0.78)\nthe model is only a transformer model based on the public kernels\n2.1. By increasing the number of parameters, the performance of a single model can be increased to around 0.8 (0.78->0.8) in public LB.\n2.1.1. 3 layers of transformer with the embedding size 480.\nPreprocessing by mean and std of single sign sequence\n3.1. the preprocessing does affect the final performance.\n3.1.1. we tried different ways of calculating the mean and std and found out that using the mean and std of the single sign sequence results in better cv.\nFeature engineering like distances between points\n4.1. we selected and used around 106 points (as the public notebook by Heck).\n4.2. distances withinpoints of hands/nose/eyes/… are calculated.\nsome methods to prevent overfitting like awp, random mask of frames, ema, etc …\nmany thanks to my teammates\n@qiaoshiji\n@zengzhaoyang\nThe source code for training models can be found here :\nhttps://github.com/zhouyuanzhe/kaggleasl5thplacesolution\n\n\n-----------METADATA-----------\n\n{'link': 'https://www.kaggle.com/c/asl-signs/discussion/406491', 'place': 5, 'competition_name': 'Google - Isolated Sign Language Recognition', 'prize': '$100,000', 'team': '1,165', 'kind': 'Research', 'metric': 'PostProcessorKernelDesc', 'year': 2023, 'nm': 406491, 'num_tokens': 473, 'methods': \"['Augmentation', 'Transformer model', 'Preprocessing', 'Feature engineering', 'Overfitting prevention']\", 'cleaned_methods': 'Post-processing'}\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"EMBEDDING_MODEL_NAME = \"BAAI/bge-base-en-v1.5\"\nCHUNK_SIZE = 512 # We choose a chunk size adapted to our model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:44:06.010661Z","iopub.execute_input":"2025-01-24T13:44:06.011007Z","iopub.status.idle":"2025-01-24T13:44:06.031512Z","shell.execute_reply.started":"2025-01-24T13:44:06.010971Z","shell.execute_reply":"2025-01-24T13:44:06.030524Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"%%time\n\ndef split_documents(\n    chunk_size: int,\n    knowledge_base: List[Document],\n    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n) -> List[Document]:\n    \"\"\"\n    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n    \"\"\"\n    \n    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n        AutoTokenizer.from_pretrained(tokenizer_name),\n        chunk_size=chunk_size,\n        chunk_overlap=int(chunk_size / 10),\n        add_start_index=True,\n        strip_whitespace=True,\n    )\n\n    docs_processed = []\n    for doc in knowledge_base:\n        docs_processed += text_splitter.split_documents([doc])\n\n    # Remove duplicates\n    unique_texts = {}\n    docs_processed_unique = []\n    for doc in docs_processed:\n        if doc.page_content not in unique_texts:\n            unique_texts[doc.page_content] = True\n            docs_processed_unique.append(doc)\n\n    return docs_processed_unique\n\nchunked_docs = split_documents(\n    CHUNK_SIZE,  \n    docs_subset,\n    tokenizer_name=EMBEDDING_MODEL_NAME,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:44:06.032405Z","iopub.execute_input":"2025-01-24T13:44:06.032691Z","iopub.status.idle":"2025-01-24T13:44:37.885978Z","shell.execute_reply.started":"2025-01-24T13:44:06.032668Z","shell.execute_reply":"2025-01-24T13:44:37.885046Z"}},"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (956 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 31.7 s, sys: 12.8 ms, total: 31.7 s\nWall time: 31.8 s\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"%%time\n\nembedding_model = HuggingFaceEmbeddings(\n    model_name=EMBEDDING_MODEL_NAME,\n    multi_process=False,\n    model_kwargs={\"device\": \"cuda\"},\n    encode_kwargs={\"normalize_embeddings\": True},  # set True for cosine similarity\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:44:37.887311Z","iopub.execute_input":"2025-01-24T13:44:37.887686Z","iopub.status.idle":"2025-01-24T13:44:38.718172Z","shell.execute_reply.started":"2025-01-24T13:44:37.887652Z","shell.execute_reply":"2025-01-24T13:44:38.717211Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 250 ms, sys: 98.2 ms, total: 348 ms\nWall time: 826 ms\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"num_docs = 5 # Default number of documents to retrieve\n\nbm25_retriever = BM25Retriever.from_documents(\n    chunked_docs\n    ).configurable_fields(\n    k=ConfigurableField(\n        id=\"search_kwargs_bm25\",\n        name=\"k\",\n        description=\"The search kwargs to use\",\n    )\n)\n\nfaiss_vectorstore = FAISS.from_documents(\n    chunked_docs, embedding_model, distance_strategy=DistanceStrategy.COSINE\n)\n\nfaiss_retriever = faiss_vectorstore.as_retriever(\n    search_kwargs={\"k\": num_docs}\n    ).configurable_fields(\n    search_kwargs=ConfigurableField(\n        id=\"search_kwargs_faiss\",\n        name=\"Search Kwargs\",\n        description=\"The search kwargs to use\",\n    )\n)\n\n# initialize the ensemble retriever\nvector_database = EnsembleRetriever(\n    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5] # You can adjust the weight of each retriever in the EnsembleRetriever\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:44:38.719129Z","iopub.execute_input":"2025-01-24T13:44:38.719481Z","iopub.status.idle":"2025-01-24T13:44:45.675902Z","shell.execute_reply.started":"2025-01-24T13:44:38.719444Z","shell.execute_reply":"2025-01-24T13:44:45.674861Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"print(data.iloc[42, :])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:44:45.677416Z","iopub.execute_input":"2025-01-24T13:44:45.677775Z","iopub.status.idle":"2025-01-24T13:44:45.684678Z","shell.execute_reply.started":"2025-01-24T13:44:45.677742Z","shell.execute_reply":"2025-01-24T13:44:45.683298Z"}},"outputs":[{"name":"stdout","text":"link                https://www.kaggle.com/c/asl-signs/discussion/...\nplace                                                               5\ncompetition_name          Google - Isolated Sign Language Recognition\nprize                                                        $100,000\nteam                                                            1,165\nkind                                                         Research\nmetric                                        PostProcessorKernelDesc\nyear                                                             2023\nnm                                                             406491\nnum_tokens                                                        473\nmethods             ['Augmentation', 'Transformer model', 'Preproc...\ncleaned_methods                                       Post-processing\nLLM_context         Competition Name: Google - Isolated Sign Langu...\nName: 42, dtype: object\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"user_query = \"\"\"\nI want to understand the 5th-place solution in the 'Google - Isolated Sign Language Recognition' competition. \nWhat overfitting prevention techniques were used, and how did they ensure model robustness?\n\"\"\"\nconfig = {\"configurable\": {\"search_kwargs_faiss\": {\"k\": 5}, \"search_kwargs_bm25\": 5}}\nretrieved_docs = vector_database.invoke(user_query, config=config)\nprint(\"----------------------Top document content----------------------\")\nprint(retrieved_docs[0].page_content)\nprint(\"----------------------Top document metadata----------------------\")\nprint(retrieved_docs[0].metadata)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:44:45.685927Z","iopub.execute_input":"2025-01-24T13:44:45.686330Z","iopub.status.idle":"2025-01-24T13:44:45.731506Z","shell.execute_reply.started":"2025-01-24T13:44:45.686282Z","shell.execute_reply":"2025-01-24T13:44:45.730440Z"}},"outputs":[{"name":"stdout","text":"----------------------Top document content----------------------\nCompetition Name: Google - Isolated Sign Language Recognition,\nPlace: 5,\nMethods Used: ['Augmentation', 'Transformer model', 'Preprocessing', 'Feature engineering', 'Overfitting prevention'],\nSolution: Here is a quick overview of the 5th-place solution.\nwe applied various augmentations like flip, concatenation, etc\n1.1. By applying different augmentations, we can increase the cv by ~ 0.02 (0.76 -> 0.78)\nthe model is only a transformer model based on the public kernels\n2.1. By increasing the number of parameters, the performance of a single model can be increased to around 0.8 (0.78->0.8) in public LB.\n2.1.1. 3 layers of transformer with the embedding size 480.\nPreprocessing by mean and std of single sign sequence\n3.1. the preprocessing does affect the final performance.\n3.1.1. we tried different ways of calculating the mean and std and found out that using the mean and std of the single sign sequence results in better cv.\nFeature engineering like distances between points\n4.1. we selected and used around 106 points (as the public notebook by Heck).\n4.2. distances withinpoints of hands/nose/eyes/… are calculated.\nsome methods to prevent overfitting like awp, random mask of frames, ema, etc …\nmany thanks to my teammates\n@qiaoshiji\n@zengzhaoyang\nThe source code for training models can be found here :\nhttps://github.com/zhouyuanzhe/kaggleasl5thplacesolution\n----------------------Top document metadata----------------------\n{'link': 'https://www.kaggle.com/c/asl-signs/discussion/406491', 'place': 5, 'competition_name': 'Google - Isolated Sign Language Recognition', 'prize': '$100,000', 'team': '1,165', 'kind': 'Research', 'metric': 'PostProcessorKernelDesc', 'year': 2023, 'nm': 406491, 'num_tokens': 473, 'methods': \"['Augmentation', 'Transformer model', 'Preprocessing', 'Feature engineering', 'Overfitting prevention']\", 'cleaned_methods': 'Transformer model', 'start_index': 0}\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:44:45.732712Z","iopub.execute_input":"2025-01-24T13:44:45.733075Z","iopub.status.idle":"2025-01-24T13:44:46.502958Z","shell.execute_reply.started":"2025-01-24T13:44:45.733045Z","shell.execute_reply":"2025-01-24T13:44:46.501760Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"\n\npage_contents = [doc.page_content for doc in retrieved_docs]  # keep only the text\nrelevant_docs = reranker.rerank(user_query, page_contents, k=5)\nrelevant_docs = [doc[\"content\"] for doc in relevant_docs]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:44:46.504041Z","iopub.execute_input":"2025-01-24T13:44:46.504338Z","iopub.status.idle":"2025-01-24T13:44:46.686216Z","shell.execute_reply.started":"2025-01-24T13:44:46.504316Z","shell.execute_reply":"2025-01-24T13:44:46.685224Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n100%|██████████| 1/1 [00:00<00:00,  6.89it/s]\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"\n\nprint(relevant_docs[0])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:44:46.687357Z","iopub.execute_input":"2025-01-24T13:44:46.687732Z","iopub.status.idle":"2025-01-24T13:44:46.692750Z","shell.execute_reply.started":"2025-01-24T13:44:46.687698Z","shell.execute_reply":"2025-01-24T13:44:46.691674Z"}},"outputs":[{"name":"stdout","text":"Competition Name: Google - Isolated Sign Language Recognition,\nPlace: 5,\nMethods Used: ['Augmentation', 'Transformer model', 'Preprocessing', 'Feature engineering', 'Overfitting prevention'],\nSolution: Here is a quick overview of the 5th-place solution.\nwe applied various augmentations like flip, concatenation, etc\n1.1. By applying different augmentations, we can increase the cv by ~ 0.02 (0.76 -> 0.78)\nthe model is only a transformer model based on the public kernels\n2.1. By increasing the number of parameters, the performance of a single model can be increased to around 0.8 (0.78->0.8) in public LB.\n2.1.1. 3 layers of transformer with the embedding size 480.\nPreprocessing by mean and std of single sign sequence\n3.1. the preprocessing does affect the final performance.\n3.1.1. we tried different ways of calculating the mean and std and found out that using the mean and std of the single sign sequence results in better cv.\nFeature engineering like distances between points\n4.1. we selected and used around 106 points (as the public notebook by Heck).\n4.2. distances withinpoints of hands/nose/eyes/… are calculated.\nsome methods to prevent overfitting like awp, random mask of frames, ema, etc …\nmany thanks to my teammates\n@qiaoshiji\n@zengzhaoyang\nThe source code for training models can be found here :\nhttps://github.com/zhouyuanzhe/kaggleasl5thplacesolution\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_instruct_2b_en\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:44:46.693708Z","iopub.execute_input":"2025-01-24T13:44:46.693968Z","iopub.status.idle":"2025-01-24T13:45:51.259365Z","shell.execute_reply.started":"2025-01-24T13:44:46.693948Z","shell.execute_reply":"2025-01-24T13:45:51.258547Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"\n\n%%time\ndisplay(Markdown(gemma_lm.generate(\"Hi, what can you tell me about Kaggle competitions?\", max_length=256)))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:45:51.260345Z","iopub.execute_input":"2025-01-24T13:45:51.260651Z","iopub.status.idle":"2025-01-24T13:46:15.821974Z","shell.execute_reply.started":"2025-01-24T13:45:51.260626Z","shell.execute_reply":"2025-01-24T13:46:15.820940Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Hi, what can you tell me about Kaggle competitions?\n\n**What are Kaggle competitions?**\n\nKaggle competitions are a platform where data scientists and machine learning engineers can participate in a wide range of data science and machine learning challenges. These competitions offer a unique opportunity to learn from experts, solve real-world problems, and potentially win prizes.\n\n**Key features of Kaggle competitions:**\n\n* **Real-world datasets:** Competitions typically use real-world datasets that are relevant to various industries and domains.\n* **Multiple data modalities:** Competitions allow participants to submit solutions for various data modalities, including images, text, and time series.\n* **Various challenge levels:** Competitions offer different challenge levels to cater to different skill sets and experience levels.\n* **Community engagement:** Kaggle provides a vibrant community where participants can interact, share knowledge, and collaborate on solutions.\n* **Prizes and recognition:** Winners of Kaggle competitions receive significant prizes and recognition, including cash, prizes, and public acclaim.\n\n**Benefits of participating in Kaggle competitions:**\n\n* **Learn from industry experts:** Solve real-world problems and gain insights from data science and machine learning experts.\n* **Boost your resume:** Winning a Kaggle competition can significantly enhance your"},"metadata":{}},{"name":"stdout","text":"CPU times: user 23.9 s, sys: 605 ms, total: 24.5 s\nWall time: 24.6 s\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"prompt_template = \"\"\"\nBased on your extensive knowledge and the following detailed context, \nplease provide a comprehensive answer to explain concepts from Kaggle competition solution write-ups:\n\nCONTEXT:\n{context}\n\nQUESTION:\n{question}\n\nANSWER:\n\"\"\"\n\nRAG_PROMPT_TEMPLATE = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=prompt_template,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:46:15.822865Z","iopub.execute_input":"2025-01-24T13:46:15.823144Z","iopub.status.idle":"2025-01-24T13:46:15.828438Z","shell.execute_reply.started":"2025-01-24T13:46:15.823121Z","shell.execute_reply":"2025-01-24T13:46:15.827344Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"def answer_with_rag(\n    question: str,\n    llm,\n    knowledge_index: FAISS,\n    reranker: Optional[RAGPretrainedModel] = None,\n    num_retrieved_docs: int = 10,\n    num_docs_final: int = 5,\n) -> Tuple[str, List[Document]]:\n    # Gather documents with retriever\n    print(\"=> Retrieving documents...\")\n    config = {\"configurable\": {\"search_kwargs_faiss\": {\"k\": num_retrieved_docs}, \"search_kwargs_bm25\": num_retrieved_docs}}\n    relevant_docs = knowledge_index.invoke(question, config=config)\n    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n    \n    # Optionally rerank results\n    if reranker:\n        print(\"=> Reranking documents...\")\n        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n        \n    relevant_docs = relevant_docs[:num_docs_final] # Keeping only num_docs_final documents\n\n    # Build the final prompt\n    context = relevant_docs[0] # We select only the top relevant document\n    \n    final_prompt = RAG_PROMPT_TEMPLATE.format(\n        context = context,  \n        question=question\n    )\n\n    # Redact an answer\n    print(\"=> Generating answer...\")\n    answer = llm.generate(final_prompt, max_length=1024)\n\n    return answer, relevant_docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:46:15.832835Z","iopub.execute_input":"2025-01-24T13:46:15.833183Z","iopub.status.idle":"2025-01-24T13:46:15.848757Z","shell.execute_reply.started":"2025-01-24T13:46:15.833148Z","shell.execute_reply":"2025-01-24T13:46:15.847456Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"%%time\nquestion = \"\"\"I want to understand the 5th-place solution in the 'Google - Isolated Sign Language Recognition' competition. \nWhat overfitting prevention techniques were used, and how did they ensure model robustness?\n\"\"\"\nanswer, relevant_docs = answer_with_rag(question, gemma_lm, vector_database, reranker)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:46:15.850241Z","iopub.execute_input":"2025-01-24T13:46:15.850500Z","iopub.status.idle":"2025-01-24T13:46:44.555928Z","shell.execute_reply.started":"2025-01-24T13:46:15.850479Z","shell.execute_reply":"2025-01-24T13:46:44.554788Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n","output_type":"stream"},{"name":"stdout","text":"=> Retrieving documents...\n=> Reranking documents...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  4.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Generating answer...\nCPU times: user 27.5 s, sys: 1.22 s, total: 28.7 s\nWall time: 28.7 s\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"def get_gemma_answer(generated_answer: str) -> str:\n    \"\"\"Function to get Gemma answer\"\"\"\n    split = generated_answer.split(\"ANSWER:\")\n    return split[1] if len(split) > 1 else \"No answer has been generatedCliquez pour utiliser cette solution\"\n\ndisplay(Markdown(\"### Gemma Answer\"))\ndisplay(Markdown(get_gemma_answer(answer)))\ndisplay(Markdown(\"### Source docs\"))\nfor i, doc in enumerate(relevant_docs):\n    display(Markdown(f\"**Document {i}------------------------------------------------------------**\"))\n    display(Markdown(doc))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:46:44.557198Z","iopub.execute_input":"2025-01-24T13:46:44.557708Z","iopub.status.idle":"2025-01-24T13:46:44.585749Z","shell.execute_reply.started":"2025-01-24T13:46:44.557666Z","shell.execute_reply":"2025-01-24T13:46:44.584777Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Gemma Answer"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n**Overfitting prevention techniques used in the 5th-place solution:**\n\n* **Random masking of frames:** This technique randomly selects a subset of frames from the training data and trains the model on this subset. This helps to prevent the model from overfitting to the specific training data and improves itsgeneralizability.\n* **Early stopping:** This technique stops training the model when it reaches a certain number of epochs or when the validation loss starts to increase. This helps to prevent the model from overfitting to the training data and improves itsgeneralizability.\n* **Data augmentation:** This technique is used to increase the size of the training dataset and to introduce diversity into the training data. This helps to prevent the model from overfitting to the training data and improves itsgeneralizability.\n* **Mean and standard deviation of the single sign sequence:** This technique is used to pre-process the training data and to improve the performance of the model.\n\n**How these techniques ensured model robustness:**\n\n* **Random masking of frames:** This technique helped to prevent the model from overfitting to the specific training data by exposing it to a wide range of images.\n* **Early stopping:** This technique helped to prevent the model from overfitting to the training data by stopping training when it reached a certain number of epochs.\n* **Data augmentation:** This technique helped to increase the size of the training dataset and to introduce diversity into the training data. This helped to prevent the model from overfitting to the training data and improved itsgeneralizability.\n* **Mean and standard deviation of the single sign sequence:** This technique helped to improve the performance of the model by reducing overfitting and by introducing diversity into the training data."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Source docs"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Document 0------------------------------------------------------------**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Competition Name: Google - Isolated Sign Language Recognition,\nPlace: 5,\nMethods Used: ['Augmentation', 'Transformer model', 'Preprocessing', 'Feature engineering', 'Overfitting prevention'],\nSolution: Here is a quick overview of the 5th-place solution.\nwe applied various augmentations like flip, concatenation, etc\n1.1. By applying different augmentations, we can increase the cv by ~ 0.02 (0.76 -> 0.78)\nthe model is only a transformer model based on the public kernels\n2.1. By increasing the number of parameters, the performance of a single model can be increased to around 0.8 (0.78->0.8) in public LB.\n2.1.1. 3 layers of transformer with the embedding size 480.\nPreprocessing by mean and std of single sign sequence\n3.1. the preprocessing does affect the final performance.\n3.1.1. we tried different ways of calculating the mean and std and found out that using the mean and std of the single sign sequence results in better cv.\nFeature engineering like distances between points\n4.1. we selected and used around 106 points (as the public notebook by Heck).\n4.2. distances withinpoints of hands/nose/eyes/… are calculated.\nsome methods to prevent overfitting like awp, random mask of frames, ema, etc …\nmany thanks to my teammates\n@qiaoshiji\n@zengzhaoyang\nThe source code for training models can be found here :\nhttps://github.com/zhouyuanzhe/kaggleasl5thplacesolution"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Document 1------------------------------------------------------------**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Competition Name: Google - Isolated Sign Language Recognition,\nPlace: 8,\nMethods Used: ['Transformer models', 'FFN encoder', 'Cosine schedule', 'Dropout', 'Label smoothing', 'Sequence cutout augmentation', 'Mirror left augmentation', 'Random rotate augmentation', 'Linear interpolation', 'Min-max normalization', 'Mean/std normalization', 'Time shift delta features', 'Angle features', 'Point to point distances', 'Tflite conversion', 'Speed up with model.half().float()', 'Normalizing points across the whole sequence', 'Mixup (tried but did not work)', 'CNNs with mixup (tried but did not work)'],\nSolution: Here is a quick overview of the 8th place solution.\n3 transformers models, 2 layers each (384 hidden, 512 hidden ffn), with an ffn encoder (512->384), trained from scratch. LR 8e-4 with cosine schedule trained for ~300 epochs, dropout 0.1, batch size 1024, label smoothing 0.1. Using hands, lips and pose (above waist only). On one transformer all pose and a subset of lips were used for diversity.\nAugmentations\nmost important was sequence cutout. On each sample, and each body part (left hand, right hand, lips, pose) with a 0.4 proba convert to nan 5 random slices of 0.15 x SequenceLength. It was hard to overfit with this in.\nmirror left\nrandom rotate.\nPreprocessing\nLinear interpolation of longer sequences to max length of 96.\nNormalise each body part, using min max - I found this better than mean/std. In one model I used mean/std for diversity.\nCreate time shift delta features on a subset of points, using time shifts of\n[1, 2, 3, 4, 6, 8, 12, 16]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Document 2------------------------------------------------------------**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Competition Name: Google - Isolated Sign Language Recognition,\nPlace: 6,\nMethods Used: ['MLP', 'Encoder', 'Transformer', 'Convolutional Neural Network (CNN)', 'Data Augmentation', 'Cross Entropy Loss', 'Weight Decay', 'Mean Teacher', 'Knowledge Distillation', 'Ensemble Learning', 'Stratified K-fold', 'Baseline Model', 'Deberta', 'Max Pooling', 'Normalization', 'Interpolation', 'Manifold Mixup', 'Face CutMix', 'Outlier Sample Mining (OUSM)', 'Model Soup', 'Data Relabeling', 'Data Truncation', 'Mish Activation Function'],\nSolution: Thanks to both, the organizers of this competition who offered a fun yet challenging problem as well as all of the other competitors - well done to everyone who worked hard for small incremental increases.\nAlthough I am the one posting the topic, this is the result of a great team effort, so big shoutout to\n@christofhenkel\n.\nBrief Summary\nOur solution is a 2 model ensemble of a MLP-encoder-frame-transformer model. We pushed our transformer models close to the limit and implemented a lot of tricks to climb up to 6th place.\nI have 1403 hours of experiment monitoring time in April (that’s 48h per day :)).\nUpdate :\nCode is available here :\nhttps://github.com/TheoViel/kaggle_islr\nDetailed Summary\nPreprocessing & Model\nPreprocessing\nRemove frames without fingers\nStride the sequence (use 1 every n frames) such that the sequence size is\n<= max_len\n. We used\nmax_len=25\nand\n80\nin the final ensemble"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Document 3------------------------------------------------------------**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Competition Name: Google - Isolated Sign Language Recognition,\nPlace: 26,\nMethods Used: ['Mixup', 'Mirroring', 'LLaMa-inspired architecture', 'RMSNorm normalization', 'Lion optimizer', 'Cosine decay learning rate', 'Batch size 128', 'Dropout 0.1', 'Exponential moving average of weights'],\nSolution: Github with all the code used\nSummary\nThe most important part of the solution is the data utilization. Major improvements were from keypoints choice and mixup. External data does not help because it is from a very different distribution. Given data amount does not benefit larger models so ensembles of small models is the way to utilize given constraints to the fullest.\nMost augmentations are not helpful, because they prevent model from learning the true data distribution. So only used mirroring and mixup (0.5).\nInputs to the model\nAll models are trained to support sequences of up to 512 frames.\nPreprocessing\nOnly 2d coordinates are used as 3rd dimension leads to unstable training.\nTo normalize inputs all keypoints are shifted so that head is located at the origin.\nScaling did not provide any benefit so not used.\nAll nans are replaced with 0 after normalization.\nChosen keypoints\nAll (21) hand keypoints\n26 face keypoints\n17 pose keypoints\nArchitecture\nLLaMa-inspired architecture. Most notable improvement comes from much better normalization RMSNorm.\nFor all models head dimensions are set to 64\nSingle model (Private/Public LB: 0.8543689/0.7702471)\n6 heads 5 layers 9.2M parameters\nEnsemble of 3 models (Private/Public LB: 0.8584568/0.7725324)\n2 heads 6 layers 1.7M parameters per model\nLarger models could be fit into file size limit, but it would time out during submission.\nAugmentations"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Document 4------------------------------------------------------------**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Competition Name: Google - Isolated Sign Language Recognition,\nPlace: 11,\nMethods Used: ['Ensemble', 'Strong augmentation', 'Manual model conversion from pytorch to tensorflow', 'CLIP transformer architecture', 'Decrease parameter size', 'Motion features', 'Longer epoch'],\nSolution: Thank you to the organizer and Kaggle for hosting this interesting challenge.\nEspecially I enjoyed this strict inference time restriction. It keeps model size reasonable and requires us for some practical technique.\nTL;DR\nEnsemble 5 transformer models\nStrong augmentation\nManual model conversion from pytroch to tensorflow\nCode is available here ->\nhttps://github.com/bamps53/kaggle-asl-11th-place-solution\nOverview\nI started from\n@hengck23\n‘s\ngreat discussion\nand\nnotebook\n. Thanks for sharing a lot of useful tricks as always!\nThe changes I made are following;\nChange model architecture to CLIP transformer in HuggingFace\nDecrease parameter size to maximize latency within the range of same accuracy\nSome strong augmentations\nHorizontal flip(p=0.5)\nRandom 3d rotation(p=1, -45~45)\nRandom scale(p=1, 0.5~1.5)\nRandom shift(p=1, 0.7~1.3)\nRandom mask frames(p=1, mask_ratio=0.5)\nRandom resize (p=1, 0.5~1.5)\nAdd motion features\ncurrent - prev\nnext - current\nVelocity\nLonger epoch, 250 for 5 fold and 300 for all data\nFor the details, please refer to the code.(planning to upload)\nModel conversion"},"metadata":{}}],"execution_count":46}]}